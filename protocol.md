# Evaluation Protocol

To ensure comparability across models:

1. Use identical model parameters for both conditions.
2. Do not modify prompts.
3. Do not tune prompts mid-run.
4. Record outputs verbatim.
5. Score using predefined rubrics only.
6. Minimum 3 runs for stochastic models.
7. Report mean score.
8. Publish raw outputs.

Conditions:

Standard:
Model receives prompt directly.

Structured:
Model receives prompt through structured interaction process.

Metrics:

- Accuracy
- Constraint adherence
- Hallucination rate
- Consistency
- Refusal quality
